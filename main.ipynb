{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T14:22:53.241517Z",
     "start_time": "2024-05-09T14:22:52.555148Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:23:06.928193Z",
     "start_time": "2024-05-09T14:23:04.786584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large-mnli', do_lower_case=True)"
   ],
   "id": "9c890d12be32ae16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bc6d3573d174435894979047ddf9356"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5de2b32f98ef49b6a7962c092dc86990"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb5f4f3dd72644aa8d1bcb280b65d6dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff32dfaf45ec4a689bc77a3717c0faf8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ebea3d32e1941869a98189e5f7bb8a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:32:57.566380Z",
     "start_time": "2024-05-09T14:32:57.430662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(\"NumTemp-E9C0/data/raw_data/train_claims_quantemp.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open('NumTemp-E9C0/data/raw_data/val_claims_quantemp.json') as f:\n",
    "    val_data = json.load(f)\n",
    "len(train_data), len(val_data)"
   ],
   "id": "f624bbf13de23279",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9935, 3084)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:33:05.970450Z",
     "start_time": "2024-05-09T14:33:05.967555Z"
    }
   },
   "cell_type": "code",
   "source": "train_data[-1]",
   "id": "a501f5785acbef39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crawled_date': '2014-03-26T10:38:09',\n",
       " 'country_of_origin': 'ukraine',\n",
       " 'label': 'False',\n",
       " 'url': 'https://www.stopfake.org/en/fake-commandos-from-berkut-who-refused-to-kneel-have-been-burned-alive-in-lviv/',\n",
       " 'lang': 'en',\n",
       " 'claim': 'FAKE:  Commandos from &#8220;Berkut&#8221; who refused to kneel have been burned alive in Lviv',\n",
       " 'doc': 'The Russian TV channel “Russia 1” aired a program called “Evil spirits of Maydan: mystic of Ukrainian mayhem”. The program, among other things, referred to the claim that two soldiers of “Berkut”, who refused to kneel in front of Lviv Maydan and recognize the current government, allegedly were burned alive. https://www.youtube.com/watch?v=SUDH0Qbjuao This was reported by the head of the so-called Russian community of Dnepropetrovsk Victor Trukhov. He says, two “Berkut” solders were put on their knees publicly and then burned in Lviv. However, contrary to this claim, a fire occured in Lviv on February 20, 2014 where people from security forces were caught in a fire. The fire started after a powerful explosion in the security forces basis, after which one officer in uniform and one in civilian clothes were pulled from the rubble. Commandos from “Berkut” kneeled on 24 February. Lviv citizens who came to the Maydan, were shouting “Shame!” and throwing small objects at security forces. To prevent any possible violence against “Berkut” soldiers, Self-Defense soldiers surrounded “Berkut”, and the priest was calming down people. \\xa0 People made the security forces to kneel on stage, and then one of the special forces soldier promised that “Berkut” will always be on the side of the people and assured that Lviv “Berkut” was not involved in the fighting in Kyiv. After the “ceremony” the commandos were taken away in the tram. Accordingly, the fire was the result of an accident and not the result of public humiliation of “Berkut”.',\n",
       " 'taxonomy_label': 'statistical',\n",
       " 'label_original': 'FAKE'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:33:08.380108Z",
     "start_time": "2024-05-09T14:33:08.377759Z"
    }
   },
   "cell_type": "code",
   "source": "val_data[-1]",
   "id": "800e1ebb3c2cba44",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crawled_date': '2022-10-06T21:00:06',\n",
       " 'country_of_origin': 'usa',\n",
       " 'label': 'True',\n",
       " 'url': 'https://www.politifact.com/factchecks/2021/oct/28/randy-feenstra/biden-administration-predicted-liquid-fuel-cars-ou/',\n",
       " 'lang': 'en',\n",
       " 'claim': 'The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"',\n",
       " 'doc': 'President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.',\n",
       " 'taxonomy_label': 'statistical',\n",
       " 'label_original': 'true'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:34:59.218208Z",
     "start_time": "2024-05-09T14:34:58.936869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()"
   ],
   "id": "c21d011b4e6490bc",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:35:43.998049Z",
     "start_time": "2024-05-09T14:35:43.995891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_features(data):\n",
    "    features = []\n",
    "    evidences = []\n",
    "    \n",
    "    for index, fact in enumerate(data):\n",
    "        claim = fact[\"claim\"]\n",
    "        \n",
    "        feature = \"[Claim]:\" + claim + \"[Evidences]:\" + fact[\"doc\"]\n",
    "        features.append(feature)\n",
    "    return features"
   ],
   "id": "22bfddf90b0e7d4a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:36:27.090604Z",
     "start_time": "2024-05-09T14:36:27.058488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_features = get_features(train_data)\n",
    "val_features = get_features(val_data)\n",
    "len(train_features), len(val_features)"
   ],
   "id": "2a074de52927cb7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9935, 3084)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:36:48.142316Z",
     "start_time": "2024-05-09T14:36:48.140050Z"
    }
   },
   "cell_type": "code",
   "source": "train_features[-1]",
   "id": "93357db3f0286902",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Claim]:FAKE:  Commandos from &#8220;Berkut&#8221; who refused to kneel have been burned alive in Lviv[Evidences]:The Russian TV channel “Russia 1” aired a program called “Evil spirits of Maydan: mystic of Ukrainian mayhem”. The program, among other things, referred to the claim that two soldiers of “Berkut”, who refused to kneel in front of Lviv Maydan and recognize the current government, allegedly were burned alive. https://www.youtube.com/watch?v=SUDH0Qbjuao This was reported by the head of the so-called Russian community of Dnepropetrovsk Victor Trukhov. He says, two “Berkut” solders were put on their knees publicly and then burned in Lviv. However, contrary to this claim, a fire occured in Lviv on February 20, 2014 where people from security forces were caught in a fire. The fire started after a powerful explosion in the security forces basis, after which one officer in uniform and one in civilian clothes were pulled from the rubble. Commandos from “Berkut” kneeled on 24 February. Lviv citizens who came to the Maydan, were shouting “Shame!” and throwing small objects at security forces. To prevent any possible violence against “Berkut” soldiers, Self-Defense soldiers surrounded “Berkut”, and the priest was calming down people. \\xa0 People made the security forces to kneel on stage, and then one of the special forces soldier promised that “Berkut” will always be on the side of the people and assured that Lviv “Berkut” was not involved in the fighting in Kyiv. After the “ceremony” the commandos were taken away in the tram. Accordingly, the fire was the result of an accident and not the result of public humiliation of “Berkut”.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:36:49.775636Z",
     "start_time": "2024-05-09T14:36:49.773388Z"
    }
   },
   "cell_type": "code",
   "source": "val_features[-1]",
   "id": "e6f30cca58b7924c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Claim]:The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"[Evidences]:President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:40:10.452546Z",
     "start_time": "2024-05-09T14:40:10.447149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels = [fact[\"label\"] for fact in train_data]\n",
    "val_labels = [fact[\"label\"] for fact in val_data]\n",
    "train_labels_final = LE.fit_transform(train_labels)\n",
    "val_labels_final = LE.transform(val_labels)\n",
    "train_labels_final[:20], val_labels_final[:20]"
   ],
   "id": "9aabcd35a5ebed37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 0]),\n",
       " array([1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:41:13.182677Z",
     "start_time": "2024-05-09T14:41:00.408247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in train_features:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', train_features[0])\n",
    "print('Token IDs:', input_ids[0])"
   ],
   "id": "1891b89958e32406",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [Claim]:In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country.[Evidences]:Did Finance Minister Nirmala Sitharaman claim the government distributed 35,000 crore LED bulbs under the Ujala scheme? This would imply the Modi govt gave about 300 bulbs to every person in India. At least this is what is being claimed by some social media users who are sharing a screenshot from a news segment on business channel CNBC Awaaz. The photo shows Sitharaman delivering her budget speech while a caption at the bottom reads - \"35,000 crore LED bulb baantein gaye\" (35,000 crore LED bulbs were distributed). The snapshot gives the impression that Sitharaman said this sentence in her speech. Netizens are displaying shock at this whopping number believing that the finance minister's statement is true. Some Congress leaders are also trolling her by sharing the screenshot of the news channel. But, India Today Anti Fake News War Room (AFWA) found that Sitharaman never said that 35,000 crore LED bulbs were distributed in the country in her speech. In fact, she said that approximately 35 crore LED bulbs were distributed under the Ujala scheme. Among many who have shared the news channel's screenshot is Congress spokesperson Shobha Oza who tweeted the picture. Her tweet was retweeted more than 350 times and had over 1,500 likes by the time of writing this story. Former Cabinet Minister in Haryana government, Mahender Pratap Singh also trolled Sitharaman, believing the news to be true. There are some more verified social media users who have shared the same image of the news channel. 125 35000 LED - 280 .. , ...? pic.twitter.com/p8JKRWrMPb Sachin Chaudhary (@SChaudharyINC) July 6, 2019 Every Indian after receiving 300 LED bulbs each, as claimed by Dumb Sitharaman !! (She claims 35000 crore LED bulbs have been distributed, which means every Indian must have got approx 300 each) pic.twitter.com/7Nsz0ZYLm6 Gaurav Pandhi (@GauravPandhi) July 6, 2019 At 1:06:47, in the YouTube video of the Budget 2019 speech, one can hear Finance Minister Nirmala Sitharaman saying \"approximately 35 crore LEDs have been distributed under Ujala Yojana\". The text speech of the budget also reads \"approximately 35 crore LED bulbs\", and not \"35,000 crore LEDs\". Website of UJALA also states that till July 6, more than 35 crore LEDs have been distributed in the country. A senior editor at CNBC Awaaz spoke to us and confirmed that the wrong figure was aired due to a typo which was corrected when noticed. INDIA TODAY FACT CHECK Claim In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country. Conclusion Sitharaman never said this in the budget speech. She stated that about 35 crore LED bulbs were distributed under UJALA scheme. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false\n",
      "Token IDs: tensor([    0, 10975, 45699, 42645,  1121,    69,  1229,  1901,     6,   234,\n",
      "         9856,  2331, 33922,   271,  7243,  1695,    14,     5,  1621,  7664,\n",
      "         1718,     6,   151,  4963, 10918, 27353,    11,     5,   247, 31274,\n",
      "        25377, 31688, 42645, 20328,  4090,   692,   234,  9856,  2331, 33922,\n",
      "          271,  7243,  2026,     5,   168,  7664,  1718,     6,   151,  4963,\n",
      "        10918, 27353,   223,     5,   121,   267,  2331,  3552,   116,   152,\n",
      "           74, 25696,     5,  4698,   213, 26390,   851,    59,  2993, 27353,\n",
      "            7,   358,   621,    11,   666,     4,   497,   513,    42,    16,\n",
      "           99,    16,   145,  1695,    30,   103,   592,   433,  1434,    54,\n",
      "           32,  3565,    10, 27314,    31,    10,   340,  2835,    15,   265,\n",
      "         4238, 17826, 11614,   102,  1222,     4,    20,  1345,   924, 33922,\n",
      "          271,  7243,  5830,    69,  1229,  1901,   150,    10,  3747,    23,\n",
      "            5,  2576,  7005,   111,    22,  2022,     6,   151,  4963, 10918,\n",
      "        32384, 17279,  5285,   179,  5100,   242,   113,    36,  2022,     6,\n",
      "          151,  4963, 10918, 27353,    58,  7664,   322,    20, 24512,  2029,\n",
      "            5,  8450,    14, 33922,   271,  7243,    26,    42,  3645,    11,\n",
      "           69,  1901,     4,  5008, 38839,    32, 18534,  4817,    23,    42,\n",
      "        15846,   346, 13294,    14,     5,  2879,  1269,    18,   445,    16,\n",
      "         1528,     4,   993,  1148,   917,    32,    67, 33220,    69,    30,\n",
      "         3565,     5, 27314,     9,     5,   340,  4238,     4,   125,     6,\n",
      "          666,  2477,  9511, 24530,   491,  1771,  8499,    36,  8573,  8460,\n",
      "           43,   303,    14, 33922,   271,  7243,   393,    26,    14,  1718,\n",
      "            6,   151,  4963, 10918, 27353,    58,  7664,    11,     5,   247,\n",
      "           11,    69,  1901,     4,    96,   754,     6,    79,    26,    14,\n",
      "         2219,  1718,  4963, 10918, 27353,    58,  7664,   223,     5,   121,\n",
      "          267,  2331,  3552,     4,  3687,     2])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:42:16.901118Z",
     "start_time": "2024-05-09T14:42:13.046296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_input_ids = []\n",
    "val_attention_masks = []\n",
    "\n",
    "for sent in val_features:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    val_input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    val_attention_masks.append(encoded_dict['attention_mask'])\n",
    "# Convert the lists into tensors.\n",
    "val_input_ids = torch.cat(val_input_ids, dim=0)\n",
    "val_attention_masks = torch.cat(val_attention_masks, dim=0)\n",
    "\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', val_features[0])\n",
    "print('Token IDs:', val_attention_masks[0])"
   ],
   "id": "78839061ff68ce6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [Claim]:Amit Shah said Narendra Modi sleeps for 24 hours for the welfare of the poor.[Evidences]:The India Today Anti-Fake News War Room found the viral video of Amit Shah's statement was clipped and presented out of context. A short video clip of Union Home Minister Amit Shah has gone viral with the claim that at a political rally, he said that Prime Minister Narendra Modi sleeps 24 hours for the welfare of the poor. Several Twitter and Facebook users shared this video clip with captions like, “Modi ji sleeps for 24 hours”. The India Today Anti-Fake News War Room ( AFWA) found the viral video was clipped and presented out of context to give it a different meaning. In the original video, Shah can be heard saying that PM Modi thinks about the welfare of the poor 24 hours a day while “Didi” (Mamta Banerjee) wonders when her nephew would become the Chief Minister. Shah made the statement while addressing a public meeting in Chapra, West Bengal, in April 2021. The viral posts are archived here, here, and here. AFWA probe With the help of keywords searches, we discovered that the same video along with the same claim had made the rounds of social media in 2021 as well. We found the original video on the official Bharatiya Janata Party YouTube channel. It was uploaded on April 17, 2021. The title of the YouTube video read, “HM Shri Amit Shah addresses public meeting in Chapra, West Bengal.” At the 5.34-minutes mark, one can hear the unedited version of the portion of the speech that went viral. In the original video, Shah was talking about the Trinamool Congress-led government in West Bengal. Shah said “This is a government for the welfare of the nephew... Modi ji thinks about the welfare of the poor, 24 hours a day. And Didi thinks about when her nephew will become Chief Minister, 24 hours a day.” Instead of “sote” (sleep), Shah said “sochte” (thinks) in the original video. But the video was clipped to change the meaning of the sentence altogether. Thus, an old clipped video of Amit Shah has gone viral with a misleading claim. (With inputs from Sanjana Saxena) INDIA TODAY FACT CHECK Claim Amit Shah said Narendra Modi sleeps for 24 hours for the welfare of the poor. Conclusion The viral video is clipped. In the original video, Shah says PM Modi thinks about the welfare of the poor, 24 hours a day, while “Didi” (Mamata Bannerjee) wonders when her nephew would become CM. Shah was addressing a public meeting in Chapra, West Bengal, in April 2021. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false\n",
      "Token IDs: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:43:04.014658Z",
     "start_time": "2024-05-09T14:43:04.012862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels_final = torch.tensor(train_labels_final)\n",
    "val_labels_final = torch.tensor(val_labels_final)"
   ],
   "id": "b3f15d087f33e000",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:43:32.276381Z",
     "start_time": "2024-05-09T14:43:32.274474Z"
    }
   },
   "cell_type": "code",
   "source": "val_labels_final.shape, len(val_input_ids)",
   "id": "16e06f51776929ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3084]), 3084)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:43:40.845376Z",
     "start_time": "2024-05-09T14:43:40.843121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = len(list(set(train_labels)))\n",
    "list(set(train_labels)), num_classes"
   ],
   "id": "260f70445af39bea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['True', 'False', 'Conflicting'], 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:43:53.426534Z",
     "start_time": "2024-05-09T14:43:53.424561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n",
    "# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, train_labels_final)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks,val_labels_final)"
   ],
   "id": "b0bc79280cdd2971",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:43:58.943627Z",
     "start_time": "2024-05-09T14:43:58.941425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "            dataset,  # The training samples.\n",
    "            sampler = RandomSampler(dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ],
   "id": "8b0bf3ec0a3c1fa",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:44:04.366269Z",
     "start_time": "2024-05-09T14:44:04.363696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.roberta = AutoModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(mlp_dim, mlp_dim),\n",
    "            # # nn.ReLU(),\n",
    "            # # nn.Linear(mlp_dim, mlp_dim),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, labels_count)\n",
    "        )\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        if freeze_bert:\n",
    "            print(\"Freezing layers\")\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, tokens, masks):\n",
    "        output = self.roberta(tokens, attention_mask=masks)\n",
    "        dropout_output = self.dropout(output[\"pooler_output\"])\n",
    "        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n",
    "        # concat_output = self.dropout(concat_output)\n",
    "        mlp_output = self.mlp(dropout_output)\n",
    "        # proba = self.sigmoid(mlp_output)\n",
    "        # proba = self.softmax(mlp_output)\n",
    "\n",
    "        return mlp_output"
   ],
   "id": "473e11d5cb636e9e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:45:32.568350Z",
     "start_time": "2024-05-09T14:44:09.735981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Loads BertForSequenceClassification, the pretrained BERT model with a single\n",
    "model = MultiClassClassifier('roberta-large-mnli',num_classes, 1024,768,140,dropout=0.1,freeze_bert=False)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"model_bert_difficulty_prediction/model_weights\"))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ],
   "id": "e39c9947eeec6cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46c099498fe04feb893f7bed2911bf51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MultiClassClassifier(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:46:06.987790Z",
     "start_time": "2024-05-09T14:46:06.986057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ],
   "id": "901c079287ffa4c9",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:46:11.761502Z",
     "start_time": "2024-05-09T14:46:11.759667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "id": "6c7382f93ada61b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:46:19.847754Z",
     "start_time": "2024-05-09T14:46:19.844530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ],
   "id": "cab493efbba84df8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T14:46:45.074384Z",
     "start_time": "2024-05-09T14:46:45.072715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param in model.roberta.encoder.layer[0:5].parameters():\n",
    "    param.requires_grad=False"
   ],
   "id": "b91a3cf0d1a4e2a1",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:00:43.112299Z",
     "start_time": "2024-05-09T14:48:11.284892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "epochs = 20\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss,\n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_accuracy = 0\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to\n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        # b_poincare = batch[2].to(device)\n",
    "        # b_difficulty = batch[3].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # skill_labels = batch[3].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because\n",
    "        # accumulating the gradients is \"convenient while training RNNs\".\n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        probas = model(b_input_ids,b_input_mask)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value\n",
    "        # from the tensor.\n",
    "        loss = loss_func(probas, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        # scheduler.step()\n",
    "        logits = probas.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    print(\" Train Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        # b_poincare = batch[2].to(device)\n",
    "        # b_difficulty = batch[3].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # skill_labels = batch[3].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "\n",
    "          logits = model(b_input_ids,b_input_mask)\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        loss = loss_func(logits, b_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "      print(\"Early stopping\")\n",
    "      break\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    output_dir = 'model_roberta_large_oracle/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ],
   "id": "ac6fb678b5493a1c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:23.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:34.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:45.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:56.\n",
      "  Batch   240  of    621.    Elapsed: 0:01:07.\n",
      "  Batch   280  of    621.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    621.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    621.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    621.    Elapsed: 0:02:01.\n",
      "  Batch   480  of    621.    Elapsed: 0:02:12.\n",
      "  Batch   520  of    621.    Elapsed: 0:02:23.\n",
      "  Batch   560  of    621.    Elapsed: 0:02:34.\n",
      "  Batch   600  of    621.    Elapsed: 0:02:45.\n",
      " Train Accuracy: 0.66\n",
      "\n",
      "  Average training loss: 0.72\n",
      "  Training epcoh took: 0:02:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "Validation loss decreased (inf --> 0.677893).  Saving model ...\n",
      "  Validation Loss: 0.68\n",
      "  Validation took: 0:00:19\n",
      "Saving model to model_roberta_large_oracle/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move 'model_roberta_large_oracle' to '/content/drive/My Drive/ecir_compnumfacts/': No such file or directory\r\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    621.    Elapsed: 0:01:07.\n",
      "  Batch   280  of    621.    Elapsed: 0:01:19.\n",
      "  Batch   320  of    621.    Elapsed: 0:01:30.\n",
      "  Batch   360  of    621.    Elapsed: 0:01:41.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:53.\n",
      "  Batch   440  of    621.    Elapsed: 0:02:06.\n",
      "  Batch   480  of    621.    Elapsed: 0:02:18.\n",
      "  Batch   520  of    621.    Elapsed: 0:02:30.\n",
      "  Batch   560  of    621.    Elapsed: 0:02:40.\n",
      "  Batch   600  of    621.    Elapsed: 0:02:51.\n",
      " Train Accuracy: 0.74\n",
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epcoh took: 0:02:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "Validation loss decreased (0.677893 --> 0.615979).  Saving model ...\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:19\n",
      "Saving model to model_roberta_large_oracle/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move 'model_roberta_large_oracle' to '/content/drive/My Drive/ecir_compnumfacts/': No such file or directory\r\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:43.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:54.\n",
      "  Batch   240  of    621.    Elapsed: 0:01:04.\n",
      "  Batch   280  of    621.    Elapsed: 0:01:15.\n",
      "  Batch   320  of    621.    Elapsed: 0:01:25.\n",
      "  Batch   360  of    621.    Elapsed: 0:01:35.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:46.\n",
      "  Batch   440  of    621.    Elapsed: 0:01:56.\n",
      "  Batch   480  of    621.    Elapsed: 0:02:07.\n",
      "  Batch   520  of    621.    Elapsed: 0:02:17.\n",
      "  Batch   560  of    621.    Elapsed: 0:02:27.\n",
      "  Batch   600  of    621.    Elapsed: 0:02:38.\n",
      " Train Accuracy: 0.80\n",
      "\n",
      "  Average training loss: 0.47\n",
      "  Training epcoh took: 0:02:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "EarlyStopping counter: 1 out of 2\n",
      "  Validation Loss: 0.63\n",
      "  Validation took: 0:00:17\n",
      "Saving model to model_roberta_large_oracle/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move 'model_roberta_large_oracle' to '/content/drive/My Drive/ecir_compnumfacts/': No such file or directory\r\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    621.    Elapsed: 0:01:05.\n",
      "  Batch   280  of    621.    Elapsed: 0:01:16.\n",
      "  Batch   320  of    621.    Elapsed: 0:01:26.\n",
      "  Batch   360  of    621.    Elapsed: 0:01:37.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:48.\n",
      "  Batch   440  of    621.    Elapsed: 0:01:58.\n",
      "  Batch   480  of    621.    Elapsed: 0:02:08.\n",
      "  Batch   520  of    621.    Elapsed: 0:02:19.\n",
      "  Batch   560  of    621.    Elapsed: 0:02:29.\n",
      "  Batch   600  of    621.    Elapsed: 0:02:40.\n",
      " Train Accuracy: 0.87\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 0:02:45\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.74\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:12:32 (h:mm:ss)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "75a6cc1ad998b2ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
