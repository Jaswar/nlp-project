{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 8430660,
     "sourceType": "datasetVersion",
     "datasetId": 4993738
    },
    {
     "sourceId": 8430777,
     "sourceType": "datasetVersion",
     "datasetId": 5020750
    },
    {
     "sourceId": 8431344,
     "sourceType": "datasetVersion",
     "datasetId": 5021191
    },
    {
     "sourceId": 8433644,
     "sourceType": "datasetVersion",
     "datasetId": 5022952
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# If there's a GPU available...\n",
    "model_dir = \"sileod/deberta-v3-base-tasksource-nli\"\n",
    "tokenizer_dir = \"sileod/deberta-v3-base-tasksource-nli\"\n",
    "model_type = \"bert\"\n",
    "\n",
    "input_dimension = 768\n",
    "mlp_dim = 500\n",
    "dropout = 0.1\n",
    "freeze = False\n",
    "vocal = False\n",
    "small_dataset = False\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "batch_size = batch_size if not small_dataset else 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:37.133477Z",
     "iopub.execute_input": "2024-05-16T18:45:37.133907Z",
     "iopub.status.idle": "2024-05-16T18:45:37.142658Z",
     "shell.execute_reply.started": "2024-05-16T18:45:37.133873Z",
     "shell.execute_reply": "2024-05-16T18:45:37.141602Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:54.723982Z",
     "start_time": "2024-06-09T16:23:54.162283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer,AutoModel, BertTokenizer\n",
    "import json\n",
    "\n",
    "# Load the BART tokenizer.\n",
    "print(f'Loading {tokenizer_dir} tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, do_lower_case=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:37.826228Z",
     "iopub.execute_input": "2024-05-16T18:45:37.826580Z",
     "iopub.status.idle": "2024-05-16T18:45:38.346400Z",
     "shell.execute_reply.started": "2024-05-16T18:45:37.826553Z",
     "shell.execute_reply": "2024-05-16T18:45:38.345311Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:55.131548Z",
     "start_time": "2024-06-09T16:23:54.724756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sileod/deberta-v3-base-tasksource-nli tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Changed from NumTemp-E9C0/output/bm25_top_100_train\n",
    "with open(\"../NumTemp-E9C0/output/programfc_bm25_top_100_train_reordered.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open('../NumTemp-E9C0/output/programfc_bm25_top_100_val_reordered.json') as f:\n",
    "    val_data = json.load(f)\n",
    "with open('../NumTemp-E9C0/output/programfc_bm25_top_100_test_reordered.json') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "if small_dataset:\n",
    "    train_data = train_data[:10]\n",
    "    val_data = val_data[:10]\n",
    "    test_data = test_data[:10]\n",
    "    \n",
    "len(train_data), len(val_data), len(test_data)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:38.349740Z",
     "iopub.execute_input": "2024-05-16T18:45:38.350069Z",
     "iopub.status.idle": "2024-05-16T18:45:40.491099Z",
     "shell.execute_reply.started": "2024-05-16T18:45:38.350036Z",
     "shell.execute_reply": "2024-05-16T18:45:40.490190Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:55.852065Z",
     "start_time": "2024-06-09T16:23:55.132096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9935, 3084, 2495)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "if vocal:\n",
    "    train_data[-1]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.492298Z",
     "iopub.execute_input": "2024-05-16T18:45:40.492661Z",
     "iopub.status.idle": "2024-05-16T18:45:40.496964Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.492625Z",
     "shell.execute_reply": "2024-05-16T18:45:40.496064Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:55.854609Z",
     "start_time": "2024-06-09T16:23:55.852661Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "if vocal:\n    val_data[-1]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.499346Z",
     "iopub.execute_input": "2024-05-16T18:45:40.499623Z",
     "iopub.status.idle": "2024-05-16T18:45:40.507663Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.499599Z",
     "shell.execute_reply": "2024-05-16T18:45:40.506855Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:55.865046Z",
     "start_time": "2024-06-09T16:23:55.855388Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "from sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\nk = 100",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.508744Z",
     "iopub.execute_input": "2024-05-16T18:45:40.509102Z",
     "iopub.status.idle": "2024-05-16T18:45:40.517907Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.509069Z",
     "shell.execute_reply": "2024-05-16T18:45:40.517004Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:56.088510Z",
     "start_time": "2024-06-09T16:23:55.865390Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features(data):\n",
    "    features = []\n",
    "    evidences = []\n",
    "    \n",
    "    for index, fact in enumerate(data):\n",
    "        claim = fact[\"claim\"]\n",
    "        feature = '[Claim:]' + claim \n",
    "        if 'predicted_programs' in fact:\n",
    "            programs = fact['predicted_programs'][0]\n",
    "            programs = ' '.join(programs)\n",
    "            feature += '[Program:]' + programs\n",
    "        evidences = fact['top_n'][:k]\n",
    "        evidences = ''.join(f'[Evidence]:{e}' for e in evidences)\n",
    "        feature += evidences\n",
    "        features.append(feature)\n",
    "    return features"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.519071Z",
     "iopub.execute_input": "2024-05-16T18:45:40.519365Z",
     "iopub.status.idle": "2024-05-16T18:45:40.528639Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.519340Z",
     "shell.execute_reply": "2024-05-16T18:45:40.527834Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:56.091812Z",
     "start_time": "2024-06-09T16:23:56.089102Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "train_features = get_features(train_data)\nval_features = get_features(val_data)\ntest_features = get_features(test_data)\nlen(train_features), len(val_features), len(test_features)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.529688Z",
     "iopub.execute_input": "2024-05-16T18:45:40.529942Z",
     "iopub.status.idle": "2024-05-16T18:45:40.546775Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.529920Z",
     "shell.execute_reply": "2024-05-16T18:45:40.545812Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:56.355388Z",
     "start_time": "2024-06-09T16:23:56.092215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9935, 3084, 2495)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "print(train_features[-1])\n",
    "if vocal:\n",
    "    train_features[-1]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.547937Z",
     "iopub.execute_input": "2024-05-16T18:45:40.548244Z",
     "iopub.status.idle": "2024-05-16T18:45:40.555686Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.548220Z",
     "shell.execute_reply": "2024-05-16T18:45:40.554801Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:56.358061Z",
     "start_time": "2024-06-09T16:23:56.356063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Claim:]\"For perspective, Special Session costs you $50k, every single day. That’s roughly the average teacher’s salary in NC.\"[Program:]fact_1 = Verify(\"Special Session costs $50k every single day.\") fact_2 = Verify(\"The average teacher's salary in NC is roughly $50k.\") label = Predict(fact_1 and fact_2)[Evidence]:13 sept. 2018  nationwide, the estimated average public-school teacher's salary is now $58,950, according to the national center for education statisticsa ...[Evidence]:average weight of students and teacher = 160 kg. formula used: average = ( sum of all observations / total number of observations ). calculations: as per the ...[Evidence]:the average weight of 13 students and their teacher = 24.5 kg.  total weight of 13 students and their teacher = 24.5  (13 + 1) = 343 kg.[Evidence]:other state's crime lab average costs and completed cases/kits. state average laboratory processing time = 11 days. average quality review[Evidence]:in 39 states, the average teacher's salary declined relative to inflation between the 2010 and 2016 school years (the latest year with[Evidence]:texas christian university, fort worth, one-year tuition = $46,950. baylor university, waco, one-year tuition = $45,542. these are tuition only costs and do not ...[Evidence]:di a gollwitzer  citato da 81  ... time of study completion. (april 9th, 2020). participants' average daily step-count april 3-9, 2020, m = 3,916.23, sd = 4,218.21, functioned as the outcome ...[Evidence]:gross salary = rs 8,00,000  rs 50,000 = rs 7,50,000. the gross salary deducts the professional tax of rs 2,400 a year (this is the professional tax in ...[Evidence]:pers plan 2 formula. 2% x service credit years x average final compensation = monthly benefit. example: let's say you work 23 years and the average of your ...[Evidence]:percent of students in florida scoring at naep. proficient level. in 2022. = 30 in 219. = 28 in 1998. = 18, significantly different from the state's results in ...1 pagina[Evidence]:in comparison, the bls estimated that the average compensation costs for manufacturing jobs in china totaled roughly $1.36 per hour in 2008. this is a marked ...[Evidence]:how many days is a school year? the average days in the school year in the ... texas. no requirement. 1,260. utah. 180. kindergarten = 450. grade 1 = 810. grades ...[Evidence]:de y ohara  2022  cit 3 fois  each cost was calculated assuming that one mask was consumed per day. these costs were converted to us dollars. us $1 = 110 yen. *1 unit price average was ...[Evidence]:cituojant s prinja  2016  cituoja 94  annual costs of delivering health care services at primary and community health centers in north india. annual cost, phc(n = 7), chc(n = 7).[Evidence]:for a given period, the capped salary limit = cap amount * percent effort. total ... period personnel cost = total requested salary + total benefits on salary ...[Evidence]:28 giu 2022  globally, walmart serves 37 million customers every single day. in fact, just one walmart supercenter in the u.s. serves an average of ...[Evidence]:2022-05-16  the average food bill for a child alone costs parents around 1,545.14 per year, averaging them roughly 27,000 over 18 years. healthcare costs ...[Evidence]:in another example, say you have been approached with an opportunity for a new job and they offer you a yearly wage of $60,000 instead of your current $50,000 salary. what is the percentage increase from the difference of $10,000? we plug the numbers into the first formula above to get $60,000 / $50,000 * 100 - 100 = 1.2 * 100 - 100 = 120 - 100 = 20% increase. you can verify this using this online percent increase calculator. first, consider an example of a salary percentage increase. say your salary is $50,000 and you were offered a salary hike amounting to a 20% increase of your current pay. to calculate your new salary after the raise do: $50,000 + $50,000 * 20 / 100 = $50,000 + $50,000 * 0.2 = $50,000 + $10,000 = $60,000.[Evidence]:28 juin 2022  burger king serves roughly 15.7 million customers every single day. that's over 5.7 billion customers served every year (equal to ...[Evidence]:30 janv. 2022  36.6k people helped. report flag outlined. answer: answer is 135 students. step-by-step explanation: idli = 30. dosa = 40. parantha = 15. poori ...[Evidence]:trend analyses using segmented regression (figure) indicated that during june 1july 2, 2020, the covid-19 7-day rolling average incidence increased each day in both counties that ultimately had mask mandates in place (mean increase = 0.25 cases per 100,000 per day; 95% confidence interval [ci] = 0.170.33) and counties that did not (mean increase = 0.08 cases per 100,000 per day; 95% ci = 0.010.14). after the governors executive order, covid-19 incidence decreased each day in mandated counties (mean decrease = 0.08 cases per 100,000 per day; 95% ci = 0.14 to 0.03); in nonmandated counties, incidence continued to increase each day (mean increase = 0.11 cases per 100,000 per day; 95% ci = 0.010.21).[Evidence]:20 mar 2018  total students (100 %) - passed the exam (72%) = failed (28%) which is equal to 392. so, 28 % is = 392 then. 72 % which will be = 392*72/28 = 1008 total number ...[Evidence]:in 2022, the average score of eighth-grade students in wisconsin was 262. ... in 2019, the average score. = 262 for the nation and. 267 for wisconsin; both scores.1 page[Evidence]:by a ross  2012  cited by 134  note: percentiles for frequency of home yoga practice: least = 0 days/month, 25th = 4 days/month, 50th = 12 days/month, 75th = 20 days/month, ...[Evidence]:the average salary in south africa is zar 26,032 a month. what is the average salary in cape town? it is hard to pinpoint an average salary in cape ...[Evidence]:it would be reported in slip as: florida = $5,000 and non-florida = $4,000. premium is taxed at 4.94%, regardless of the state in which there is exposure. any ...[Evidence]:2017-05-31  spendthe salary a single person needs to get by in every u.s. state ... between 1941 and 2014 americans spent money on most of the same things ...[Evidence]:27 apr 2023  women's earnings as a percentage of men's for full-time wage and salary workers by state, 2021 annual averages. (u.s. average = 83.1%). map 2.[Evidence]:27 apr 2023  women's earnings as a percentage of men's for full-time wage and salary workers by state, 2021 annual averages. (u.s. average = 83.1%). map 2 ...[Evidence]:hence, the money spent by the company on employee wages and benefits, w b = 2 x = 2  $ 26 , 000 = $ 52 , 000 .[Evidence]:reports of suicide attempts during the 12 months before the survey significantly increased among female students (pd = 2.3%; pr = 1.21) and was unchanged among male students from 2019 to 2021 (table 3). the prevalence of reported attempted suicide was 13.3% among females in 2021 and 6.6% among males. increases in reports of suicide attempts occurred among white (pd = 3.0%; pr = 1.32), 10th grade (pd = 4.6%; pr = 1.41) female students, as well as among female students with opposite sex only sexual contacts (pd = 4.7%, pr = 1.41). in 2021, black female students were more likely (pr = 1.43) than white female students to report having attempted suicide, as well as 9th and 10th grade (pr = 1.54 and 1.52 respectively) female students compared with 12th grade, and lgbq+ students (pr = 1.86 lesbian or gay; 3.26 bisexual; 1.53 questioning; 2.47 other) compared with heterosexual students. also in 2021, female students reporting opposite sex only sexual contacts (pr = 2.50) and those with same sex only or both sex partners (pr = 5.19) were more likely than those with no sexual contact to report having attempted suicide. in 2021, ai/an and black male students reports of attempted suicide were significantly higher (pr = 2.37, 2.05 respectively) than white male students. among males in 2021, lgbq+ students (pr = 3.93 gay; 3.44, bisexual; 2.81 questioning; 6.60 other) were more likely to have reported attempting suicide compared with heterosexual students, and male students reporting opposite sex only sexual contacts (pr = 2.96) and those with same sex only or both sex partners were more likely (pr = 10.72) than those with no sexual contact to report having attempted suicide. approximately one third (30.0%) of female students in 2021 reported that they had seriously considered attempting suicide during the 12 months before the survey, a substantial increase compared with 2019 (24.1%) (table 1). the percentage of male students reporting that they had seriously considered attempting suicide were similar during the study period (2019 = 13.3%; 2021 = 14.3%). increases in seriously considered attempting suicide differed by race, grade, and sex of sexual contacts among female students. for example, the prevalence significantly increased among black (pd = 6.8%; pr = 1.29), hispanic (pd = 6.0%; pr = 1.27), and white (pd = 7.1%; pr = 1.29) female students. among male students, although the overall prevalence were similar, increases were observed among hispanic (pd = 2.8%; pr = 1.24), 11th-grade (pd = 3.2%; pr = 1.23), opposite sex only sexual contacts (pd = 4.0%; pr = 1.27), and same sex or both sex sexual contacts (pd = 18.2%; pr = 1.47). the prevalence of attempted suicide that required medical treatment was relatively stable between 2019 and 2021 for female (2019 = 3.3%; 2021 = 3.9%) and male (2019 = 1.7%; 2021 = 1.7%) students overall and by student characteristics (table 4). in 2021, female hispanic students were more likely (pr = 1.31) than female white students, and 9th grade female students (pr = 1.51) were more likely than 12th grade students, to report an attempted suicide that required medical treatment. bisexual and other identifying female students were more likely (pr = 4.23 and 3.04, respectively) than heterosexual female students to report an attempted suicide that required medical treatment. black (pr = 2.64) and hispanic (pr = 1.61) male students were more likely than white male students to report an attempted suicide that required medical treatment. among males in 2021, lgbq+ students (pr = 7.93 gay; 7.23 bisexual; 7.06 questioning; 14.31 other) were more likely to report an attempted suicide that required medical treatment compared with heterosexual students. students with opposite sex only sexual contacts (pr = 5.08 female; 4.09 male) and those with same sex or both sexes sexual contacts were more likely (pr = 14.36 female; 30.15 male) than students with no sexual contact to report an attempted suicide that required medical treatment.[Evidence]:reports of suicide attempts during the 12 months before the survey significantly increased among female students (pd = 2.3%; pr = 1.21) and was unchanged among male students from 2019 to 2021 (table 3). the prevalence of reported attempted suicide was 13.3% among females in 2021 and 6.6% among males. increases in reports of suicide attempts occurred among white (pd = 3.0%; pr = 1.32), 10th grade (pd = 4.6%; pr = 1.41) female students, as well as among female students with opposite sex only sexual contacts (pd = 4.7%, pr = 1.41). in 2021, black female students were more likely (pr = 1.43) than white female students to report having attempted suicide, as well as 9th and 10th grade (pr = 1.54 and 1.52 respectively) female students compared with 12th grade, and lgbq+ students (pr = 1.86 lesbian or gay; 3.26 bisexual; 1.53 questioning; 2.47 other) compared with heterosexual students. also in 2021, female students reporting opposite sex only sexual contacts (pr = 2.50) and those with same sex only or both sex partners (pr = 5.19) were more likely than those with no sexual contact to report having attempted suicide. in 2021, ai/an and black male students reports of attempted suicide were significantly higher (pr = 2.37, 2.05 respectively) than white male students. among males in 2021, lgbq+ students (pr = 3.93 gay; 3.44, bisexual; 2.81 questioning; 6.60 other) were more likely to have reported attempting suicide compared with heterosexual students, and male students reporting opposite sex only sexual contacts (pr = 2.96) and those with same sex only or both sex partners were more likely (pr = 10.72) than those with no sexual contact to report having attempted suicide. approximately one third (30.0%) of female students in 2021 reported that they had seriously considered attempting suicide during the 12 months before the survey, a substantial increase compared with 2019 (24.1%) (table 1). the percentage of male students reporting that they had seriously considered attempting suicide were similar during the study period (2019 = 13.3%; 2021 = 14.3%). increases in seriously considered attempting suicide differed by race, grade, and sex of sexual contacts among female students. for example, the prevalence significantly increased among black (pd = 6.8%; pr = 1.29), hispanic (pd = 6.0%; pr = 1.27), and white (pd = 7.1%; pr = 1.29) female students. among male students, although the overall prevalence were similar, increases were observed among hispanic (pd = 2.8%; pr = 1.24), 11th-grade (pd = 3.2%; pr = 1.23), opposite sex only sexual contacts (pd = 4.0%; pr = 1.27), and same sex or both sex sexual contacts (pd = 18.2%; pr = 1.47). in 2021, asian female students had a lower prevalence of seriously considered attempting suicide compared with white female students (pr = 0.77). the prevalence of female students in 9th, 10th, and 11th grade who seriously considered attempting suicide was significantly greater (pr = 1.20, 1.31, and 1.16, respectively) than female students in 12th grade. in addition, prevalence was significantly higher among lgbq+ female students (pr = 2.06 lesbian or gay; 2.60 bisexual; 1.80 questioning; 2.40 other) compared with heterosexual students. among males in 2021, students in 9th and 10th grade were less likely (pr = 0.72 and 0.77, respectively) than students in 12th grade to report seriously considered attempting suicide. similar to female students, prevalence was significantly higher among lgbq+ male students (pr = 3.05 gay; 3.45 bisexual; 2.40 questioning; 3.93 other) compared with heterosexual students.[Evidence]:power & fuel = [2,000 - 1,600] / 400 = ` 1. per unit variable and. fixed cost = 1,600 - 600 = ` 1000. budget for 80% capacity level. budgeted production (80 ...19 pages[Evidence]:average annual salary was $54,912 and median salary was $51,000. state retirement system of illinois average salary is 17 percent higher than usa average ...[Evidence]:(d) = +1.60. *note u = unemployed, e = employed. eg uue ... duration of unemployment. information about the unemployment duration for the persons interviewed.[Evidence]:12 lug 2022  the probability for biologic maternity (n) was calculated with n = (1 - pcum) x 100% = (1 - 0.0000134) x 100% = 99.99%. the bone fragments in ...[Evidence]:if 48.8%  15 = 7.32 =>. divide the number 7.32 by the number 15... ... and see if we get as a result: 48.8%. a quick note: 100/100 = 100  100 = 100% = 1 ...[Evidence]:2022-03-06  how many minutes in a year (common year)?  answer #1  1 common year = 365 days  1 common year = 365 days* 1440 minutes/days = 525,600 minutes.[Evidence]:2023-04-27  women's earnings as a percentage of men's for full-time wage and salary workers by state, 2021 annual averages. (u.s. average = 83.1%). map 2 ...[Evidence]:2023-08-04  today is a very special day and only happens once every thousand (1,000) years. your age + your year of birth, every person = 2023. it's so ...[Evidence]:given average of monthly income of a person = rs 10000 increased salary of one person in family per year = rs 1,20000 formula used averag.[Evidence]:so, number of seconds in 1 hour = 60  60 = 3600 seconds.[Evidence]:for the sake of illustrating this notion, i will assume a recalibrated average iq score of  = 100 and a standard deviation of  = 15 for men and  =10 for women  their average scores are equal. with this information, we can begin to compute some boys-to-girls ratios in iq differences. mentor their above-average iq children, in the range of 115125 iq, for every two (2) boys, we have one (1) girl.[Evidence]:approximately one third (30.0%) of female students in 2021 reported that they had seriously considered attempting suicide during the 12 months before the survey, a substantial increase compared with 2019 (24.1%) (table 1). the percentage of male students reporting that they had seriously considered attempting suicide were similar during the study period (2019 = 13.3%; 2021 = 14.3%). increases in seriously considered attempting suicide differed by race, grade, and sex of sexual contacts among female students. for example, the prevalence significantly increased among black (pd = 6.8%; pr = 1.29), hispanic (pd = 6.0%; pr = 1.27), and white (pd = 7.1%; pr = 1.29) female students. among male students, although the overall prevalence were similar, increases were observed among hispanic (pd = 2.8%; pr = 1.24), 11th-grade (pd = 3.2%; pr = 1.23), opposite sex only sexual contacts (pd = 4.0%; pr = 1.27), and same sex or both sex sexual contacts (pd = 18.2%; pr = 1.47). reports of suicide attempts during the 12 months before the survey significantly increased among female students (pd = 2.3%; pr = 1.21) and was unchanged among male students from 2019 to 2021 (table 3). the prevalence of reported attempted suicide was 13.3% among females in 2021 and 6.6% among males. increases in reports of suicide attempts occurred among white (pd = 3.0%; pr = 1.32), 10th grade (pd = 4.6%; pr = 1.41) female students, as well as among female students with opposite sex only sexual contacts (pd = 4.7%, pr = 1.41). in 2021, black female students were more likely (pr = 1.43) than white female students to report having attempted suicide, as well as 9th and 10th grade (pr = 1.54 and 1.52 respectively) female students compared with 12th grade, and lgbq+ students (pr = 1.86 lesbian or gay; 3.26 bisexual; 1.53 questioning; 2.47 other) compared with heterosexual students. also in 2021, female students reporting opposite sex only sexual contacts (pr = 2.50) and those with same sex only or both sex partners (pr = 5.19) were more likely than those with no sexual contact to report having attempted suicide. in 2021, ai/an and black male students reports of attempted suicide were significantly higher (pr = 2.37, 2.05 respectively) than white male students. among males in 2021, lgbq+ students (pr = 3.93 gay; 3.44, bisexual; 2.81 questioning; 6.60 other) were more likely to have reported attempting suicide compared with heterosexual students, and male students reporting opposite sex only sexual contacts (pr = 2.96) and those with same sex only or both sex partners were more likely (pr = 10.72) than those with no sexual contact to report having attempted suicide. in 2021, asian female students had a lower prevalence of seriously considered attempting suicide compared with white female students (pr = 0.77). the prevalence of female students in 9th, 10th, and 11th grade who seriously considered attempting suicide was significantly greater (pr = 1.20, 1.31, and 1.16, respectively) than female students in 12th grade. in addition, prevalence was significantly higher among lgbq+ female students (pr = 2.06 lesbian or gay; 2.60 bisexual; 1.80 questioning; 2.40 other) compared with heterosexual students. among males in 2021, students in 9th and 10th grade were less likely (pr = 0.72 and 0.77, respectively) than students in 12th grade to report seriously considered attempting suicide. similar to female students, prevalence was significantly higher among lgbq+ male students (pr = 3.05 gay; 3.45 bisexual; 2.40 questioning; 3.93 other) compared with heterosexual students.[Evidence]:de s sandbags  calculation of number of sandbags required per one foot raise in levee height. n = 3h+9h. 2. 2 when h = 1 n = 6 when h = 2 n = 21 when h = 3 n = 45 when h = 4 ...2 pages[Evidence]:our first step here is to calculate the average annual profit per customer  which is determined by deducting the two sets of costs (product costs and service costs) from the annual revenue. in this case, it is $2000  $500  $100 = $1,400.[Evidence]:4 mar 2017  welcome to the mass destruction cost curve. at the low end, a single ak-47 assault rifle costs roughly $700. customers buying in bulk, such as ...[Evidence]:let's say you work 23 years and the average of your highest 24 months of income (afc) is $5,400 per month. 2% x 23 years x $5,400 = $2,484. when you retire, you ...[Evidence]:amongst 18 to 24s, video watching is most prevalent with an average of 9.53 videos per day; 68% of adults in the uk watch online videos every single day. to ...[Evidence]:apr 29, 2011  stacey vanek smith: speaking of perspective, if you think gas prices are bad here, a gallon of gas in europe, costs roughly $9 dollars a ...[Evidence]:and at last, if we talk about the million and billion in 1 crore, then 10 million = 1 crore and 0.01 billion = 1 crore. in the indian numbering system, 100 lakhs = 1 crore, 100 crore = 1 billion. in india, pakistan, bangladesh, and nepal, 1 crore is a very big amount. firstly crore was used in sri lanka, but now a day, it is replaced by millions, billions, and trillions. 1 trillion is equal to 100lakh crore. 1,00,00,000 in indian notation. = 1,000,000,000,000 in western notation.[Evidence]:the postvaccination antibody analysis included 100 healthy volunteers, 32 fully vaccinated with moderna (median age = 31 years; median interval from second vaccine dose to blood draw = 28 days), 51 fully vaccinated with pfizer-biontech (median age = 27 years; median interval from second dose to blood draw = 27 days), and 17 fully vaccinated with janssen (median age = 31 years; median interval from vaccine dose to blood draw = 35 days). anti-rbd levels were higher in participants vaccinated with the moderna vaccine (median = 4,333; interquartile range [iqr] = 3,1347,197; geometric mean = 4,274; 95% ci = 3,3935,384 bau/ml) than in those who received the pfizer-biontech vaccine (median = 3,217; iqr = 2,0484,668; geometric mean = 2,950; 95% ci = 2,3253,742 bau/ml) (p = 0.033) or the janssen vaccine (median = 57; iqr = 2694; geometric mean = 51; 95% ci = 3090 bau/ml) (p<0.001) (figure). anti-spike igg levels in participants vaccinated with the moderna vaccine (median = 3,236; iqr = 2,1254,975, geometric mean = 3,059; 95% ci = 2,4793,774 bau/ml) did not significantly differ from those in recipients of the pfizer-biontech vaccine (median = 2,983; iqr = 1,9544,059; geometric mean = 2,444; 95% ci = 1,9363,085 bau/ml) (p = 0.217), but were significantly higher than levels in participants who received the janssen vaccine (median = 59; iqr = 30104; geometric mean = 56; 95% ci = 3297 bau/ml) (p<0.001).[Evidence]:is the average selling price right for your business? selling price = $150 + (40% x $150) heres the formula for the average selling price in action:[Evidence]:311 = meaning the ku klux klan, 11 = k, and 3 times 11 equaling \"kkk\"; 6mwe = meaning \"6 million wasn't enough\"; 21-2-12 = slogan of the unforgiven, meaning ...[Evidence]:2 weeks is equal to exactly 14 days. in scientific notation. 2 weeks. = 2 x 100 weeks. = 1.4 x 101 ...[Evidence]:19 ott 2023  the average annual premium for employer-sponsored single and family coverage increased by 7% in 2023, an increase from 2022, but roughly in ...[Evidence]:most studies (n = 36) compared protocols or registrations with full reports in clinical trials, while a single survey focused on primary studies of clinical ...[Evidence]:48.8% = 48.8/100 = 48.8  100 = 0.488. 100% = 100/100 = 100  100 = 1. what does it mean: percentage of 48.8% of which number is equal to 40? 48.8% of what ...[Evidence]:2022-01-16  population of the united states = 300,000,000 number of zip codes in the us = 10,000 number of days in a year = 300 assume also that each ...[Evidence]:oct 19, 2023  the average annual premium for employer-sponsored single and family coverage increased by 7% in 2023, an increase from 2022, but roughly in ...[Evidence]:1 billion seconds = 32 years. 1 trillion seconds = 31,688 years.[Evidence]:average fuel consumption per 100km = volume of fuel burned (in liters)/distance traveled (in km) x 100. example: a route 1000 km long, the total consumption was 50 liters. average fuel consumption per 100km = (50  100)/1000=5 l/100 km. total consumption = (distance / 100)  average. example: 5000 km route, average consumption was 8 l/100 km.[Evidence]:= 90 million atm cards in urban. ... = 2,80,000 atms are there in india. the actual number of atms in india is around ~2,40,000.[Evidence]:2 nov 2015  in 2014, according to mary meeker's annual internet trends report, people uploaded an average of 1.8 billion digital images every single day.[Evidence]:di b jenner  2002  citato da 1  [changes in average life span of monks and nuns in poland in the years 1950-2000] ... average life span of monks was 2.4 years shorter (t162 = 1.99, p = 0.047) ...[Evidence]:solution for what is 9 percent of 322: 9 percent *322 = (9:100)*322 = (9*322):100 = 2898:100 = 28.98. now we have: 9 percent of 322 = 28.98.[Evidence]:di ab brayne  2021  citato da 6  the specialties with the oldest average age at death were general practitioners (80.3, sd = 12.5, n = 2508), surgeons (79.9, sd = 13.6, n = 853) ...[Evidence]:cituojant v do  2023  cituoja 6  the states with the highest annual average counts of 8+ hour outage events were louisiana (n = 553), texas (n = 527), michigan (n = 447) ...[Evidence]:police recruit starting salary = $33.72 hourly / $70,138 annually. police officer maximum salary = $51.84 hourly / $107,827 annually . hiring bonuses.[Evidence]:2017-08-23  if your dog is currently eating 3 cups of dry food per day = 1.5 lbs. of raw per day. feeding portion guidelines: your average 12-13 lb dog ...[Evidence]:what is the average salary in new york city? how does the average salary in nyc compare? the average salary in nyc is well above the national average. the average household income in the u.s. is $91,547, according to the 2020 census acs. therefore, the $107,000 nyc average salary comes in at approximately 14% higher than the national average.[Evidence]:solution for what is 70 percent of 322: 70 percent *322 = (70:100)*322 = (70*322):100 = 22540:100 = 225.4. now we have: 70 percent of 322 = 225.4.[Evidence]:48.8% of 9510.01 = 464088.488. 48.8% of 9510.02 = 464088.976. 48.8% of 9510.03 = 464089.464 ; 48.8% of 9510.26 = 464100.688. 48.8% of 9510.27 = 464101.176. 48.8% ...[Evidence]:given: time taken by the train to travel from p and q = 1 hour 20 minutes concept used: distance/speed = time time = 1/speed calculation: let the usual sp.[Evidence]:lunch: zinger burger + french fries + drink 6,7 usd = 5.5 gbp; food in a box: rice + meat 6,1 usd = 5 gbp; chicken salad 6,1 usd = 5 gbp; burritos 6,1 usd = 5 gbp; coffee 2,1 - 2,4 usd = 1.7 - 2 gbp; various prepared food in supermarkets . sliced ham, salami 1,5 - 1,7 usd = 1.2 - 1.4 gbp per 100 gr. chicken breasts, sliced ready-made 1,8 usd ...[Evidence]:2022-05-19  ... /factcheck-thomas-bottle-idusl2n2xa2rt | date = 2022-05-19 | archiveurl = http://archive.today/w393u | archivedate = 2022-05-19 }}.[Evidence]:2 fvr. 2015  entertainment: average youtube views per video = 9,816  'parody' in the title and description generate an average of 47k views  'music' in the ...[Evidence]:2 feb 2015  entertainment: average youtube views per video = 9,816  'parody' in the title and description generate an average of 47k views  'music' in the ...[Evidence]:11 mai 2021  ... oxygen = 26%, hydrogen = 10%, sulphur = 6%, nitrogen = 3% and inerts = 20%. burning rate is 1000 tonnes/d. oxygen in air by weight is 23%.[Evidence]:jul 22, 2021  the problem is, is supposedly, upvote = adds to the conversation, downvote = detracts from conversation. in reality, upvote = i agree, ...[Evidence]:2017-10-22  it happens only once every 1,000 years. this year your age + your year of birth, each individual is = 2019. for example, you are 55 years old ...[Evidence]:18 oct. 2023  number of visitors in january = 100 thousand number of visitors in june = 260 thousand change in number in june to that in january = (260 ...[Evidence]:long-term average = 100. sep-2023. mexico long-term average = 100. household disposable incomeindicator, 5.04 gross per capita, percentage change, previous ...[Evidence]:how much is 48.8%? what number is it? 48.8% of 1100 = 536.80 48.8% of 1200 = 585.60 48.8% ...[Evidence]:2015-02-02  entertainment: average youtube views per video = 9,816  'parody' in the title and description generate an average of 47k views  'music' in the ...[Evidence]:24 janv. 2022  ... oxygen by volume is required to completely burn 10 gram of sulphur? 2s(s) + 2o2(g) = 2so2(g) + o2(g) = 2so3(g). 10g / 32.06g/mole = 0.31mole s.[Evidence]:int year = moment.year; // month gets 1 (january). int month = moment.month; // day gets ... current culture's calendar calendar thaicalendar = cultureinfo ...[Evidence]:fossil fuels are more energy dense than other sources. energy density diagram. natural gas = 53.1mj/kg; gasoline diesel = 45.8 note: mj/kg = ...[Evidence]:table 1. covid-19 deaths / 1000 inhabitants at year 1 and year 2 of the pandemic. mean annual temperature (mat), average annual hours of sunshine (aahss) and average annual uv index (aauvi) are depicted for each country. fig. (4). negative correlations between the covid-19 death numbers and several climate parameters: average annual temperature (a), average annual sunshine hours (b) and the annual average of the daily uv index (c). when covid-19 death numbers were plotted against geographic latitudes, we obtained inverted bell-shaped curves, for both the first and second year of the pandemic, with a coefficient of determination of (r 2 = 0,32) and (r 2 = 0,39), respectively. in addition, covid-19 death numbers were very negatively correlated with the average annual levels of temperature (r 2 = 0,52, p= 4.92x10 -7 ), sunshine hours (r 2 = 0,36, p= 7.68x10 -6 ) and uv index (r 2 = 0,38, p= 4.16x10 -5 ). bell-shaped curves were obtained when latitude was plotted against the average annual number of temperature, sunshine hours and uv index, with a coefficient of determination of (r 2 = 0,85), (r 2 = 0,452) and (r 2 = 0,87), respectively.[Evidence]:find the total carbohydrate, and below it find sugars. sugar is listed in grams. every 4 grams of sugar = 1 teaspoon of table sugar. in this example, ...2 pagine[Evidence]:200 inr = 2.40 usd. change 200 indian rupee = 2.40 us dollar. 1000 inr = 12.01 usd. change 1000 indian rupee = 12.01 us dollar. 2000 inr = 24.02 usd.[Evidence]:= l = tbsp. = tsp. 1 gal. 4 qt. 8 pt. 16 cups 128 fl. oz. 3.8 l.18 pagine[Evidence]:what is the percentage decrease in the population? solution: original population = 18,560. decreased population = 15,787. decrease in population = 18,560  ...[Evidence]:200 inr = 2.42 usd. convert 200 indian rupee = 2.42 us dollar. 1000 inr = 12.12 usd. convert 1000 indian rupee = 12.12 us dollar. 2000 inr = 24.23 usd.[Evidence]:exchange 200 inr = 2.40 usd. exchange 200 indian rupee = 2.40 us dollar. exchange 1000 inr = 12.01 usd. exchange 1000 indian rupee = 12.01 us dollar.[Evidence]:a year has on average 52.143 weeks = 52 weeks plus one day. every four years we have a leap year, which has 366 days. a leap year, therefore, has 52 weeks and 2 ...[Evidence]:what is the average salary for college graduates? pay & salary average salary for college graduates by industry (plus faqs) average salary for college graduates by industry (plus faqs)[Evidence]:2022-09-30  cara = comprehensive addiction and recovery act; pdmp = prescription drug monitoring program; support = substance use-disorder prevention that.[Evidence]:cobra venom (cobratoxin) is a small basic protein (mr = 7000). it contains 62 amino acids in a single chain, cross-linked by four disulfide bonds. the toxin ...[Evidence]:mues'un = pleasant aromatic; light; perfume; flowers; vanilla. mues'in = charming; delicious; dulcet; lovely; smooth; sweet. mok'on = fresh; raw; sour. mmzen ...\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "if vocal:\n    val_features[-1]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.556656Z",
     "iopub.execute_input": "2024-05-16T18:45:40.557213Z",
     "iopub.status.idle": "2024-05-16T18:45:40.565632Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.557189Z",
     "shell.execute_reply": "2024-05-16T18:45:40.564737Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:56.369613Z",
     "start_time": "2024-06-09T16:23:56.358423Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "if vocal:\n    test_features[-1]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.568506Z",
     "iopub.execute_input": "2024-05-16T18:45:40.568814Z",
     "iopub.status.idle": "2024-05-16T18:45:40.577646Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.568771Z",
     "shell.execute_reply": "2024-05-16T18:45:40.576749Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:56.377978Z",
     "start_time": "2024-06-09T16:23:56.370009Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "train_labels = [fact[\"gold\"] for fact in train_data]\n",
    "val_labels = [fact[\"gold\"] for fact in val_data]\n",
    "test_labels = [fact[\"gold\"] for fact in test_data]\n",
    "\n",
    "train_labels_final = LE.fit_transform(train_labels)\n",
    "val_labels_final = LE.transform(val_labels)\n",
    "test_labels_final = LE.transform(test_labels)\n",
    "train_labels_final[:20], val_labels_final[:20], test_labels_final[:20]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.578830Z",
     "iopub.execute_input": "2024-05-16T18:45:40.579607Z",
     "iopub.status.idle": "2024-05-16T18:45:40.591657Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.579573Z",
     "shell.execute_reply": "2024-05-16T18:45:40.590818Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:23:56.392739Z",
     "start_time": "2024-06-09T16:23:56.378388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 1, 2, 1, 1, 2, 2, 1, 1, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2]),\n",
       " array([2, 0, 2, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1, 1, 1, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "input_ids = []\nattention_masks = []\n\n# needed for gpt2. Added by Emil\nif tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n# temp change Emil\nfor sent in train_features:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 256,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        truncation=True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n\n    # Add the encoded sentence to the list.\n    input_ids.append(encoded_dict['input_ids'])\n\n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\n\n\n# Print sentence 0, now as a list of IDs.\nif vocal:\n    print('Original: ', train_features[0])\n    print('Token IDs:', input_ids[0])\nprint(\"done\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.592860Z",
     "iopub.execute_input": "2024-05-16T18:45:40.593197Z",
     "iopub.status.idle": "2024-05-16T18:45:40.782996Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.593166Z",
     "shell.execute_reply": "2024-05-16T18:45:40.782041Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:25:21.433677Z",
     "start_time": "2024-06-09T16:23:56.393151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "val_input_ids = []\nval_attention_masks = []\n# temp change Emil\nfor sent in val_features:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 256,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        truncation=True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n\n    # Add the encoded sentence to the list.\n    val_input_ids.append(encoded_dict['input_ids'])\n\n    # And its attention mask (simply differentiates padding from non-padding).\n    val_attention_masks.append(encoded_dict['attention_mask'])\n# Convert the lists into tensors.\nval_input_ids = torch.cat(val_input_ids, dim=0)\nval_attention_masks = torch.cat(val_attention_masks, dim=0)\n\n\n# Print sentence 0, now as a list of IDs.\nif vocal:\n    print('Original: ', val_features[0])\n    print('Token IDs:', val_attention_masks[0])\nprint(\"done\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:40.784640Z",
     "iopub.execute_input": "2024-05-16T18:45:40.785025Z",
     "iopub.status.idle": "2024-05-16T18:45:41.017683Z",
     "shell.execute_reply.started": "2024-05-16T18:45:40.784990Z",
     "shell.execute_reply": "2024-05-16T18:45:41.016707Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:25:48.108508Z",
     "start_time": "2024-06-09T16:25:21.435343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "test_input_ids = []\ntest_attention_masks = []\n# temp change Emil\nfor sent in test_features:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 256,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        truncation=True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n\n    # Add the encoded sentence to the list.\n    test_input_ids.append(encoded_dict['input_ids'])\n\n    # And its attention mask (simply differentiates padding from non-padding).\n    test_attention_masks.append(encoded_dict['attention_mask'])\n# Convert the lists into tensors.\ntest_input_ids = torch.cat(test_input_ids, dim=0)\ntest_attention_masks = torch.cat(test_attention_masks, dim=0)\n\n\n# Print sentence 0, now as a list of IDs.\nif vocal:\n    print('Original: ', test_features[0])\n    print('Token IDs:', test_attention_masks[0])\nprint(\"done\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.018963Z",
     "iopub.execute_input": "2024-05-16T18:45:41.019268Z",
     "iopub.status.idle": "2024-05-16T18:45:41.224923Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.019239Z",
     "shell.execute_reply": "2024-05-16T18:45:41.223812Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:09.798420Z",
     "start_time": "2024-06-09T16:25:48.109011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "# temp change Emil\ntrain_labels_final = torch.tensor(train_labels_final)\nval_labels_final = torch.tensor(val_labels_final)\ntest_labels_final = torch.tensor(test_labels_final)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.226577Z",
     "iopub.execute_input": "2024-05-16T18:45:41.226915Z",
     "iopub.status.idle": "2024-05-16T18:45:41.232684Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.226884Z",
     "shell.execute_reply": "2024-05-16T18:45:41.231582Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:09.800877Z",
     "start_time": "2024-06-09T16:26:09.798929Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "val_labels_final.shape, len(val_input_ids)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.233999Z",
     "iopub.execute_input": "2024-05-16T18:45:41.234351Z",
     "iopub.status.idle": "2024-05-16T18:45:41.243772Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.234321Z",
     "shell.execute_reply": "2024-05-16T18:45:41.242897Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:09.810964Z",
     "start_time": "2024-06-09T16:26:09.801294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3084]), 3084)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "num_classes = len(list(set(train_labels)))\nlist(set(train_labels)), num_classes",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.245080Z",
     "iopub.execute_input": "2024-05-16T18:45:41.245419Z",
     "iopub.status.idle": "2024-05-16T18:45:41.256940Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.245387Z",
     "shell.execute_reply": "2024-05-16T18:45:41.256169Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:09.818809Z",
     "start_time": "2024-06-09T16:26:09.811309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['False', 'True', 'Conflicting'], 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import TensorDataset, random_split\n# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, train_labels_final)\nval_dataset = TensorDataset(val_input_ids, val_attention_masks,val_labels_final)\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks,test_labels_final)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.257946Z",
     "iopub.execute_input": "2024-05-16T18:45:41.258246Z",
     "iopub.status.idle": "2024-05-16T18:45:41.265448Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.258209Z",
     "shell.execute_reply": "2024-05-16T18:45:41.264524Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:09.826239Z",
     "start_time": "2024-06-09T16:26:09.819175Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ntrain_dataloader = DataLoader(\n            dataset,  # The training samples.\n            sampler = RandomSampler(dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset),\n            batch_size = batch_size\n        )\n\ntest_dataloader = DataLoader(\n            test_dataset, # The validation samples.\n            sampler = SequentialSampler(test_dataset),\n            batch_size = batch_size\n        )",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.266435Z",
     "iopub.execute_input": "2024-05-16T18:45:41.266702Z",
     "iopub.status.idle": "2024-05-16T18:45:41.279347Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.266678Z",
     "shell.execute_reply": "2024-05-16T18:45:41.278445Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:09.837470Z",
     "start_time": "2024-06-09T16:26:09.826630Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "from torch import nn\nclass MultiClassClassifier(nn.Module):\n    def __init__(self, inner_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_inner_model=False):\n        super().__init__()\n\n        self.inner_model = AutoModel.from_pretrained(inner_model_path,output_hidden_states=True,output_attentions=True)\n        self.dropout = nn.Dropout(dropout)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_dim, mlp_dim),\n            nn.ReLU(),\n            # nn.Linear(mlp_dim, mlp_dim),\n            # # nn.ReLU(),\n            # # nn.Linear(mlp_dim, mlp_dim),\n            # nn.ReLU(),\n            nn.Linear(mlp_dim, labels_count)\n        )\n        # self.softmax = nn.LogSoftmax(dim=1)\n        # TODO, make such that possible with different models\n        if freeze_inner_model:\n            print(\"Freezing layers\")\n            for param in self.inner_model.parameters():\n                param.requires_grad = False\n\n    def forward(self, tokens, masks):\n        output = self.inner_model(tokens, attention_mask=masks)\n        # bart-large-mnli doesn't have \"pooler_output\" like BERT and RoBERTa do\n        # dropout_output = self.dropout(output[\"pooler_output\"])\n        last_hidden_state = output.last_hidden_state  # Get the last hidden state\n        class_token = last_hidden_state[:, 0, :]  # Get the class token\n        mlp_output = self.dropout(class_token)\n        mlp_output = self.mlp(mlp_output)\n        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n        # concat_output = self.dropout(concat_output)\n        # mlp_output = self.mlp(dropout_output)\n        # proba = self.sigmoid(mlp_output)\n        # proba = self.softmax(mlp_output)\n\n        return mlp_output",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.280446Z",
     "iopub.execute_input": "2024-05-16T18:45:41.280799Z",
     "iopub.status.idle": "2024-05-16T18:45:41.290726Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.280747Z",
     "shell.execute_reply": "2024-05-16T18:45:41.289840Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:09.845175Z",
     "start_time": "2024-06-09T16:26:09.837842Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Loads AutoModelForSequenceClassification, the pretrained BART model with a single\n",
    "model = MultiClassClassifier(model_dir,num_classes, input_dimension,mlp_dim,140,dropout=dropout,freeze_inner_model=False)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"model_bert_difficulty_prediction/model_weights\"))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.291732Z",
     "iopub.execute_input": "2024-05-16T18:45:41.292026Z",
     "iopub.status.idle": "2024-05-16T18:45:41.946538Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.292003Z",
     "shell.execute_reply": "2024-05-16T18:45:41.945552Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:10.551829Z",
     "start_time": "2024-06-09T16:26:09.845539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiClassClassifier(\n",
       "  (inner_model): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=500, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.949853Z",
     "iopub.execute_input": "2024-05-16T18:45:41.950148Z",
     "iopub.status.idle": "2024-05-16T18:45:41.955108Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.950122Z",
     "shell.execute_reply": "2024-05-16T18:45:41.954229Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:10.554182Z",
     "start_time": "2024-06-09T16:26:10.552288Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": "import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.956433Z",
     "iopub.execute_input": "2024-05-16T18:45:41.956734Z",
     "iopub.status.idle": "2024-05-16T18:45:41.964686Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.956709Z",
     "shell.execute_reply": "2024-05-16T18:45:41.963832Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:10.563414Z",
     "start_time": "2024-06-09T16:26:10.554585Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.965860Z",
     "iopub.execute_input": "2024-05-16T18:45:41.966185Z",
     "iopub.status.idle": "2024-05-16T18:45:41.977271Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.966153Z",
     "shell.execute_reply": "2024-05-16T18:45:41.976407Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:10.571192Z",
     "start_time": "2024-06-09T16:26:10.563809Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "\nif freeze:\n    if model_type == \"gpt\":\n        for param in model.inner_model.h[:-1].parameters():\n            param.requires_grad=False\n    elif model_type == \"bert\":\n        for param in model.inner_model.encoder.layer[:-1].parameters():\n            param.requires_grad = False\n    else:\n        for param in model.inner_model.layers[:-1].parameters():\n            param.requires_grad=False\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.978391Z",
     "iopub.execute_input": "2024-05-16T18:45:41.978796Z",
     "iopub.status.idle": "2024-05-16T18:45:41.991594Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.978748Z",
     "shell.execute_reply": "2024-05-16T18:45:41.990645Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:26:10.582212Z",
     "start_time": "2024-06-09T16:26:10.571608Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": "import random\nimport numpy as np\nfrom transformers import get_linear_schedule_with_warmup\n\n# This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\noptimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\nloss_func = nn.CrossEntropyLoss()\n# Total number of training steps is [number of batches] x [number of epochs].\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n\n# We'll store a number of quantities such as training and validation loss,\n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\nearly_stopping = EarlyStopping(patience=2, verbose=True)\n# For each epoch...\nfor epoch_i in range(0, epochs):\n\n    # ========================================\n    #               Training\n    # ========================================\n\n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_accuracy = 0\n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to\n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader.\n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids\n        #   [1]: attention masks\n        #   [2]: labels\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        # b_poincare = batch[2].to(device)\n        # b_difficulty = batch[3].to(device)\n        b_labels = batch[2].to(device)\n        # skill_labels = batch[3].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because\n        # accumulating the gradients is \"convenient while training RNNs\".\n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()\n\n        # Perform a forward pass (evaluate the model on this training batch).\n        probas = model(b_input_ids,b_input_mask)\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value\n        # from the tensor.\n        loss = loss_func(probas, b_labels)\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        # scheduler.step()\n        logits = probas.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        total_train_accuracy += flat_accuracy(logits, label_ids)\n    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n    print(\" Train Accuracy: {0:.2f}\".format(avg_train_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)\n\n\n\n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n\n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n\n        # Unpack this training batch from our dataloader.\n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using\n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids\n        #   [1]: attention masks\n        #   [2]: labels\n        b_input_ids = batch[0].to(device)\n\n        b_input_mask = batch[1].to(device)\n        # b_poincare = batch[2].to(device)\n        # b_difficulty = batch[3].to(device)\n        b_labels = batch[2].to(device)\n        # skill_labels = batch[3].to(device)\n\n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():\n\n            # Forward pass, calculate logit predictions.\n\n            logits = model(b_input_ids,b_input_mask)\n\n        # Accumulate the validation loss.\n        loss = loss_func(logits, b_labels)\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    early_stopping(avg_val_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n    output_dir = 'model_bart_large_oracle/'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    print(\"Saving model to %s\" % output_dir)\n    tokenizer.save_pretrained(output_dir)\n    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:41.992755Z",
     "iopub.execute_input": "2024-05-16T18:45:41.993298Z",
     "iopub.status.idle": "2024-05-16T18:45:54.460819Z",
     "shell.execute_reply.started": "2024-05-16T18:45:41.993258Z",
     "shell.execute_reply": "2024-05-16T18:45:54.459426Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:33:28.657237Z",
     "start_time": "2024-06-09T16:26:10.582668Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/pytorch/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:12.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:18.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:24.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:30.\n",
      "  Batch   240  of    621.    Elapsed: 0:00:36.\n",
      "  Batch   280  of    621.    Elapsed: 0:00:42.\n",
      "  Batch   320  of    621.    Elapsed: 0:00:48.\n",
      "  Batch   360  of    621.    Elapsed: 0:00:54.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:00.\n",
      "  Batch   440  of    621.    Elapsed: 0:01:06.\n",
      "  Batch   480  of    621.    Elapsed: 0:01:14.\n",
      "  Batch   520  of    621.    Elapsed: 0:01:20.\n",
      "  Batch   560  of    621.    Elapsed: 0:01:26.\n",
      "  Batch   600  of    621.    Elapsed: 0:01:32.\n",
      " Train Accuracy: 0.62\n",
      "\n",
      "  Average training loss: 0.83\n",
      "  Training epcoh took: 0:01:36\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.63\n",
      "Validation loss decreased (inf --> 0.798637).  Saving model ...\n",
      "  Validation Loss: 0.80\n",
      "  Validation took: 0:00:10\n",
      "Saving model to model_bart_large_oracle/\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:12.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:19.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:25.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:32.\n",
      "  Batch   240  of    621.    Elapsed: 0:00:38.\n",
      "  Batch   280  of    621.    Elapsed: 0:00:44.\n",
      "  Batch   320  of    621.    Elapsed: 0:00:51.\n",
      "  Batch   360  of    621.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:04.\n",
      "  Batch   440  of    621.    Elapsed: 0:01:11.\n",
      "  Batch   480  of    621.    Elapsed: 0:01:18.\n",
      "  Batch   520  of    621.    Elapsed: 0:01:25.\n",
      "  Batch   560  of    621.    Elapsed: 0:01:33.\n",
      "  Batch   600  of    621.    Elapsed: 0:01:39.\n",
      " Train Accuracy: 0.68\n",
      "\n",
      "  Average training loss: 0.72\n",
      "  Training epcoh took: 0:01:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "Validation loss decreased (0.798637 --> 0.763666).  Saving model ...\n",
      "  Validation Loss: 0.76\n",
      "  Validation took: 0:00:11\n",
      "Saving model to model_bart_large_oracle/\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:14.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:33.\n",
      "  Batch   240  of    621.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    621.    Elapsed: 0:00:45.\n",
      "  Batch   320  of    621.    Elapsed: 0:00:51.\n",
      "  Batch   360  of    621.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:05.\n",
      "  Batch   440  of    621.    Elapsed: 0:01:11.\n",
      "  Batch   480  of    621.    Elapsed: 0:01:18.\n",
      "  Batch   520  of    621.    Elapsed: 0:01:24.\n",
      "  Batch   560  of    621.    Elapsed: 0:01:31.\n",
      "  Batch   600  of    621.    Elapsed: 0:01:37.\n",
      " Train Accuracy: 0.74\n",
      "\n",
      "  Average training loss: 0.60\n",
      "  Training epcoh took: 0:01:40\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.65\n",
      "EarlyStopping counter: 1 out of 2\n",
      "  Validation Loss: 0.80\n",
      "  Validation took: 0:00:09\n",
      "Saving model to model_bart_large_oracle/\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    621.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    621.    Elapsed: 0:00:19.\n",
      "  Batch   160  of    621.    Elapsed: 0:00:25.\n",
      "  Batch   200  of    621.    Elapsed: 0:00:32.\n",
      "  Batch   240  of    621.    Elapsed: 0:00:38.\n",
      "  Batch   280  of    621.    Elapsed: 0:00:44.\n",
      "  Batch   320  of    621.    Elapsed: 0:00:50.\n",
      "  Batch   360  of    621.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    621.    Elapsed: 0:01:03.\n",
      "  Batch   440  of    621.    Elapsed: 0:01:09.\n",
      "  Batch   480  of    621.    Elapsed: 0:01:16.\n",
      "  Batch   520  of    621.    Elapsed: 0:01:22.\n",
      "  Batch   560  of    621.    Elapsed: 0:01:28.\n",
      "  Batch   600  of    621.    Elapsed: 0:01:34.\n",
      " Train Accuracy: 0.83\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epcoh took: 0:01:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:07:18 (h:mm:ss)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"testing\")\n",
    "\n",
    "# Tracking variables\n",
    "total_test_accuracy = 0\n",
    "total_test_loss = 0\n",
    "nb_test_steps = 0\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    # Unpack this training batch from our dataloader.\n",
    "    #\n",
    "    # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "    # the `to` method.\n",
    "    #\n",
    "    # `batch` contains three pytorch tensors:\n",
    "    #   [0]: input ids\n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels\n",
    "    b_input_ids = batch[0].to(device)\n",
    "\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    # b_poincare = batch[2].to(device)\n",
    "    # b_difficulty = batch[3].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    # skill_labels = batch[3].to(device)\n",
    "\n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        logits = model(b_input_ids,b_input_mask)\n",
    "\n",
    "    # Accumulate the validation loss.\n",
    "    loss = loss_func(logits, b_labels)\n",
    "    total_test_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences, and\n",
    "    # accumulate it over all batches.\n",
    "    total_test_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    all_predictions.extend(predictions.tolist())\n",
    "\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "avg_val_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_val_loss = total_test_loss / len(test_dataloader)\n",
    "# Measure how long the validation run took.\n",
    "\n",
    "print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "import csv\n",
    "with open('predictions.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the list as a single row in the CSV\n",
    "    writer.writerow(all_predictions)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T18:45:56.229462Z",
     "iopub.execute_input": "2024-05-16T18:45:56.230135Z",
     "iopub.status.idle": "2024-05-16T18:45:56.451432Z",
     "shell.execute_reply.started": "2024-05-16T18:45:56.230101Z",
     "shell.execute_reply": "2024-05-16T18:45:56.450525Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T16:33:35.887064Z",
     "start_time": "2024-06-09T16:33:28.658006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n",
      "  Accuracy: 0.63\n",
      "  Validation Loss: 0.89\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:33:35.889069Z",
     "start_time": "2024-06-09T16:33:35.887873Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 28
  }
 ]
}
